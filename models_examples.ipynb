{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (0.1.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from langchain) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from langchain) (3.9.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from langchain) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.9 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from langchain) (0.0.10)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.7 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from langchain) (0.1.8)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.77 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from langchain) (0.0.78)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from langchain) (1.26.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from langchain) (2.5.3)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from langchain-core<0.2,>=0.1.7->langchain) (4.2.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from langchain-core<0.2,>=0.1.7->langchain) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from openai) (0.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from openai) (2.5.3)\n",
      "Requirement already satisfied: sniffio in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: certifi in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\n",
      "Requirement already satisfied: colorama in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.12.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (4.36.2)\n",
      "Requirement already satisfied: filelock in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\src\\personal\\momento-empresa\\me-81\\speaking-ai-models\\.venv\\lib\\site-packages (from requests->transformers) (2023.11.17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "     ---------------------------------------- 0.0/232.6 kB ? eta -:--:--\n",
      "     - -------------------------------------- 10.2/232.6 kB ? eta -:--:--\n",
      "     ---- -------------------------------- 30.7/232.6 kB 445.2 kB/s eta 0:00:01\n",
      "     ----------------- ------------------ 112.6/232.6 kB 939.4 kB/s eta 0:00:01\n",
      "     -------------------------------------- 232.6/232.6 kB 1.4 MB/s eta 0:00:00\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install openai\n",
    "!pip install pandas\n",
    "!pip install transformers\n",
    "!pip install PyPDF2\n",
    "!pip install python-dotenv\n",
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def pdf_to_text(pdf_path, txt_path):\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "        with open(txt_path, 'w', encoding='utf-8') as txt_file:\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                txt_file.write(page.extract_text())\n",
    "\n",
    "pdf_to_text(\n",
    "    pdf_path='big_data_for_dummies.pdf',\n",
    "    txt_path='big_data_for_dummies.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "with open('big_data_for_dummies.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "with open('transformers.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text: str) -> int:\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap = 24,\n",
    "    length_function = count_tokens,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "config = dotenv_values('.env')\n",
    "OPEN_API_KEY = config.get('OPEN_API_KEY')\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPEN_API_KEY, model='text-embedding-ada-002')\n",
    "\n",
    "db = FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What is transformers?'\n",
    "docs = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='LLMs, or Large Language Models, are the key component behind text generation. In a nutshell, they consist of large pretrained transformer models trained to predict the next word (or, more precisely, token) given some input text. Since they predict one token at a time, you need to do something more elaborate to generate new sentences other than just calling the model — you need to do autoregressive generation.\\n\\nAutoregressive generation is the inference-time procedure of iteratively calling a model with its own generated outputs, given a few initial inputs. In 🤗 Transformers, this is handled by the generate() method, which is available to all models with generative capabilities.\\n\\nThis tutorial will show you how to:\\n\\nGenerate text with an LLM\\nAvoid common pitfalls\\nNext steps to help you get the most out of your LLM\\nBefore you begin, make sure you have all the necessary libraries installed:\\n\\nCopied\\npip install transformers bitsandbytes>=0.39.0 -q\\nGenerate text\\nA language model trained for causal language modeling takes a sequence of text tokens as input and returns the probability distribution for the next token.\\n\\n\"Forward pass of an LLM\"\\nA critical aspect of autoregressive generation with LLMs is how to select the next token from this probability distribution. Anything goes in this step as long as you end up with a token for the next iteration. This means it can be as simple as selecting the most likely token from the probability distribution or as complex as applying a dozen transformations before sampling from the resulting distribution.\\n\\n\"Autoregressive generation iteratively selects the next token from a probability distribution to generate text\"\\nThe process depicted above is repeated iteratively until some stopping condition is reached. Ideally, the stopping condition is dictated by the model, which should learn when to output an end-of-sequence (EOS) token. If this is not the case, generation stops when some predefined maximum length is reached.\\n\\nProperly setting up the token selection step and the stopping condition is essential to make your model behave as you’d expect on your task. That is why we have a GenerationConfig file associated with each model, which contains a good default generative parameterization and is loaded alongside your model.\\n\\nLet’s talk code!'),\n",
       " Document(page_content='Let’s talk code!\\n\\nIf you’re interested in basic LLM usage, our high-level Pipeline interface is a great starting point. However, LLMs often require advanced features like quantization and fine control of the token selection step, which is best done through generate(). Autoregressive generation with LLMs is also resource-intensive and should be executed on a GPU for adequate throughput.\\n\\nFirst, you need to load the model.\\n\\nCopied\\nfrom transformers import AutoModelForCausalLM\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", load_in_4bit=True\\n)\\nYou’ll notice two flags in the from_pretrained call:\\n\\ndevice_map ensures the model is moved to your GPU(s)\\nload_in_4bit applies 4-bit dynamic quantization to massively reduce the resource requirements\\nThere are other ways to initialize a model, but this is a good baseline to begin with an LLM.\\n\\nNext, you need to preprocess your text input with a tokenizer.\\n\\nCopied\\nfrom transformers import AutoTokenizer\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\")\\nmodel_inputs = tokenizer([\"A list of colors: red, blue\"], return_tensors=\"pt\").to(\"cuda\")\\nThe model_inputs variable holds the tokenized text input, as well as the attention mask. While generate() does its best effort to infer the attention mask when it is not passed, we recommend passing it whenever possible for optimal results.\\n\\nAfter tokenizing the inputs, you can call the generate() method to returns the generated tokens. The generated tokens then should be converted to text before printing.'),\n",
       " Document(page_content='Copied\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\\ntokenizer.pad_token = tokenizer.eos_token  # Most LLMs don\\'t have a pad token by default\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", load_in_4bit=True\\n)\\nGenerated output is too short/long\\nIf not specified in the GenerationConfig file, generate returns up to 20 tokens by default. We highly recommend manually setting max_new_tokens in your generate call to control the maximum number of new tokens it can return. Keep in mind LLMs (more precisely, decoder-only models) also return the input prompt as part of the output.\\n\\nCopied\\nmodel_inputs = tokenizer([\"A sequence of numbers: 1, 2\"], return_tensors=\"pt\").to(\"cuda\")\\n\\n# By default, the output will contain up to 20 tokens\\ngenerated_ids = model.generate(**model_inputs)\\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\\n\\'A sequence of numbers: 1, 2, 3, 4, 5\\''),\n",
       " Document(page_content='Copied\\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    \"HuggingFaceH4/zephyr-7b-alpha\", device_map=\"auto\", load_in_4bit=True\\n)\\nset_seed(0)\\nprompt = \"\"\"How many helicopters can a human eat in one sitting? Reply as a thug.\"\"\"\\nmodel_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\\ninput_length = model_inputs.input_ids.shape[1]\\ngenerated_ids = model.generate(**model_inputs, max_new_tokens=20)\\nprint(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])\\n\"I\\'m not a thug, but i can tell you that a human cannot eat\"\\n# Oh no, it did not follow our instruction to reply as a thug! Let\\'s see what happens when we write\\n# a better prompt and use the right template for this model (through `tokenizer.apply_chat_template`)')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nTransformers are large pretrained transformer models used for text generation. They are trained to predict the next word or token given some input text and are used for autoregressive generation. This process involves iteratively calling the model with its own generated outputs to generate new sentences. Transformers are resource-intensive and should be executed on a GPU for optimal results.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_qa_chain(OpenAI(openai_api_key=OPEN_API_KEY, temperature=0.1), chain_type='stuff')\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
